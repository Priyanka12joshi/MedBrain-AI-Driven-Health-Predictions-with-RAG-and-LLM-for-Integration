# -*- coding: utf-8 -*-
"""Untitled11.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MwZr6VEb0EtbnT_wWrBt6yE0Vc3qRc_F
"""

!pip install pipenv

import os
os.mkdir('pipenv_env')
os.chdir('pipenv_env')

!python -m pipenv install langchain langchain_community langchain_huggingface faiss-cpu
!pip install langchain langchain_community langchain_huggingface faiss-cpu pypdf

!pip install langchain_community

from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

# # Step 1: Load raw PDF(s)
# DATA_PATH ="data/"
# defload_pdf_files(data):
#   loader = load_pdf_files(data,
#                           glob ='*.pdf',
#                           loader_cls =PyPDFLoader)
#   documents =loader.load()
#   return documents
# documents =load_pdf_files(data =DATA_PATH)
# print("Lenght of PDF pages: ",len(document))
DATA_PATH="/content/pipenv_env/data"
def load_pdf_files(data):
    loader = DirectoryLoader(data,
                             glob='*.pdf',
                             loader_cls=PyPDFLoader)

    documents=loader.load()
    return documents

documents=load_pdf_files(data=DATA_PATH)
print("Length of PDF pages: ", len(documents))

from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

# Step 1: Load raw PDF(s)
DATA_PATH="/content/pipenv_env/data"
def load_pdf_files(data):
    loader = DirectoryLoader(data,
                             glob='*.pdf',
                             loader_cls=PyPDFLoader)

    documents=loader.load()
    return documents

documents=load_pdf_files(data=DATA_PATH)
print("Length of PDF pages: ", len(documents))

DATA_PATH="/content/pipenv_env/data"
def load_pdf_files(data):
    loader = DirectoryLoader(data,
                             glob='*.pdf',
                             loader_cls=PyPDFLoader)

    documents=loader.load()
    return documents

documents=load_pdf_files(data=DATA_PATH)
# print("Length of PDF pages: ", len(documents))
# step  2
def create_chunks(extracted_data):
  text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500,
                                                 chunk_overlap = 50)
  text_chunks = text_splitter.split_documnets(extracted_data)
  return text_chunks
  text_chunks = create_chunks(extracted_data)
  # print("Lenght of Text chunks: ",len(text_chunks))
# Step 2: Create Chunks
# def create_chunks(extracted_data):
#     text_splitter=RecursiveCharacterTextSplitter(chunk_size=500,
#                                                  chunk_overlap=50)
#     text_chunks=text_splitter.split_documents(extracted_data)
#     return text_chunks

# text_chunks=create_chunks(extracted_data=documents)
# #print("Length of Text Chunks: ", len(text_chunks))

from transformers import AutoTokenizer, AutoModel
import torch

def get_biobert_embedding(text):
    model_name = "dmis-lab/biobert-base-cased-v1.1"

    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModel.from_pretrained(model_name)

    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
    with torch.no_grad():
        outputs = model(**inputs)


    embedding = outputs.last_hidden_state[:, 0, :].squeeze().numpy()
    return embedding
text = "Biomedical research is advancing rapidly."
embedding = get_biobert_embedding(text)
print("BioBERT Embedding Shape:", embedding.shape)

# Step 2: Create Chunks
def create_chunks(extracted_data):
    text_splitter=RecursiveCharacterTextSplitter(chunk_size=500,
                                                 chunk_overlap=50)
    text_chunks=text_splitter.split_documents(extracted_data)
    return text_chunks

text_chunks=create_chunks(extracted_data=documents)
#print("Length of Text Chunks: ", len(text_chunks))

# Step 3: Create Vector Embeddings

def get_embedding_model():
    embedding_model=HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    return embedding_model

embedding_model=get_embedding_model()

# Step 4: Store embeddings in FAISS
DB_FAISS_PATH="vectorstore/db_faiss"
db=FAISS.from_documents(text_chunks, embedding_model)
db.save_local(DB_FAISS_PATH)

with open(".env", "w") as f:
    f.write("HF_TOKEN=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\n")

from huggingface_hub import InferenceClient

# Define the Hugging Face token (Ensure you've set it as an environment variable or manually add it)
HF_TOKEN = os.environ.get("HF_TOKEN")

# Define model ID
HUGGINGFACE_REPO_ID = "mistralai/Mistral-7B-Instruct-v0.3"

# Load the model using the Inference Client
client = InferenceClient(model=HUGGINGFACE_REPO_ID, token=HF_TOKEN)

# Function to generate response
def generate_response(prompt):
    response = client.text_generation(prompt, max_new_tokens=512, temperature=0.5)
    return response

# Example query
user_query = input("Write Query Here: ")
response = generate_response(user_query)
print("RESULT: ", response)

import os

from langchain_huggingface import HuggingFaceEndpoint
from langchain_core.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

## Uncomment the following files if you're not using pipenv as your virtual environment manager
#from dotenv import load_dotenv, find_dotenv
#load_dotenv(find_dotenv())


# Step 1: Setup LLM (Mistral with HuggingFace)
HF_TOKEN=os.environ.get("HF_TOKEN")
HUGGINGFACE_REPO_ID="mistralai/Mistral-7B-Instruct-v0.3"

def load_llm(huggingface_repo_id):
    llm=HuggingFaceEndpoint(
        repo_id=huggingface_repo_id,
        temperature=0.5,
        model_kwargs={"token":HF_TOKEN,
                      "max_length":"512"}
    )
    return llm

# Step 2: Connect LLM with FAISS and Create chain

CUSTOM_PROMPT_TEMPLATE = """
Use the pieces of information provided in the context to answer user's question.
If you dont know the answer, just say that you dont know, dont try to make up an answer.
Dont provide anything out of the given context

Context: {context}
Question: {question}

Start the answer directly. No small talk please.
"""

def set_custom_prompt(custom_prompt_template):
    prompt=PromptTemplate(template=custom_prompt_template, input_variables=["context", "question"])
    return prompt

# Load Database
DB_FAISS_PATH="vectorstore/db_faiss"
embedding_model=HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
db=FAISS.load_local(DB_FAISS_PATH, embedding_model, allow_dangerous_deserialization=True)

# Create QA chain
qa_chain=RetrievalQA.from_chain_type(
    llm=load_llm(HUGGINGFACE_REPO_ID),
    chain_type="stuff",
    retriever=db.as_retriever(search_kwargs={'k':3}),
    return_source_documents=True,
    chain_type_kwargs={'prompt':set_custom_prompt(CUSTOM_PROMPT_TEMPLATE)}
)

# Now invoke with a single query
user_query=input("Write Query Here: ")
response=qa_chain.invoke({'query': user_query})
print("RESULT: ", response["result"])
print("SOURCE DOCUMENTS: ", response["source_documents"])

import os
from huggingface_hub import InferenceClient
HF_TOKEN = os.environ.get("HF_TOKEN")
if not HF_TOKEN:
    HF_TOKEN = input("Enter your Hugging Face API token: ").strip()
HUGGINGFACE_REPO_ID = "mistralai/Mistral-7B-Instruct-v0.3"
client = InferenceClient(model=HUGGINGFACE_REPO_ID, token=HF_TOKEN)
SYSTEM_PROMPT = """
You are a highly knowledgeable AI assistant. Provide concise, fact-based answers.
Avoid speculation and focus only on verified information.
Use structured responses where applicable.
"""
def generate_response(user_query):
    """
    Generates a response from the LLM with improved accuracy.
    """
    try:
        full_prompt = f"{SYSTEM_PROMPT}\n\nUser: {user_query}\nAssistant:"
        response = client.text_generation(
            full_prompt,
            max_new_tokens=512,
            temperature=0.3,
            top_p=0.9,
            repetition_penalty=1.2,
            stream=True
        )
        final_output = ""
        for chunk in response:
            print(chunk, end="", flush=True)
            final_output += chunk

        return final_output.strip()

    except Exception as e:
        return f"Error: {str(e)}"
user_query = input("\nWrite Query Here: ")
print("\nRESULT:\n")
final_result = generate_response(user_query)

import os
from huggingface_hub import InferenceClient
HF_TOKEN = os.environ.get("HF_TOKEN")
if not HF_TOKEN:
    HF_TOKEN = input("Enter your Hugging Face API token: ").strip()
HUGGINGFACE_REPO_ID = "mistralai/Mistral-7B-Instruct-v0.3"
client = InferenceClient(model=HUGGINGFACE_REPO_ID, token=HF_TOKEN)
SYSTEM_PROMPT = """
You are a highly knowledgeable AI assistant. Provide concise, fact-based answers.
Avoid speculation and focus only on verified information.
Use structured responses where applicable.
"""
def generate_response(user_query):
    """
    Generates a response from the LLM with improved accuracy.
    """
    try:
        full_prompt = f"{SYSTEM_PROMPT}\n\nUser: {user_query}\nAssistant:"
        response = client.text_generation(
            full_prompt,
            max_new_tokens=512,
            temperature=0.3,
            top_p=0.9,
            repetition_penalty=1.2,
            stream=True
        )
        final_output = ""
        for chunk in response:
            print(chunk, end="", flush=True)
            final_output += chunk

        return final_output.strip()

    except Exception as e:
        return f"Error: {str(e)}"
user_query = input("\nWrite Query Here: ")
print("\nRESULT:\n")
final_result = generate_response(user_query)



