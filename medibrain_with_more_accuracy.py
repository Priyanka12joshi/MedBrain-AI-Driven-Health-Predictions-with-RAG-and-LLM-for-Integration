# -*- coding: utf-8 -*-
"""Medibrain with more accuracy.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11RpcJblayTLFSHIbMVMN4yTeIgIYgOCo
"""

!pip install pipenv

!pipenv --version

import os
os.mkdir('pipenv_env')
os.chdir('pipenv_env')

!python -m pipenv install langchain langchain_community langchain_huggingface faiss-cpu
!pip install langchain langchain_community langchain_huggingface faiss-cpu pypdf

!python -m pipenv run python -c "import langchain; print('LangChain Installed!')"

!pip install langchain_community

!pip install huggingface_hub
!pip install streamlit

from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

# Step 1: Load raw PDF(s)
DATA_PATH="/content/pipenv_env/data"
def load_pdf_files(data):
    loader = DirectoryLoader(data,
                             glob='*.pdf',
                             loader_cls=PyPDFLoader)

    documents=loader.load()
    return documents

documents=load_pdf_files(data=DATA_PATH)
print("Length of PDF pages: ", len(documents))

!pip install sentence-transformers transformers torch

from transformers import AutoTokenizer, AutoModel
import torch

def get_biobert_embedding(text):
    model_name = "dmis-lab/biobert-base-cased-v1.1"

    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModel.from_pretrained(model_name)

    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
    with torch.no_grad():
        outputs = model(**inputs)


    embedding = outputs.last_hidden_state[:, 0, :].squeeze().numpy()
    return embedding
text = "Biomedical research is advancing rapidly."
embedding = get_biobert_embedding(text)
print("âœ… BioBERT Embedding Shape:", embedding.shape)

from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
DATA_PATH="/content/pipenv_env/data"
def load_pdf_files(data):
    loader = DirectoryLoader(data,
                             glob='*.pdf',
                             loader_cls=PyPDFLoader)

    documents=loader.load()
    return documents

documents=load_pdf_files(data=DATA_PATH)
#print("Length of PDF pages: ", len(documents))

def create_chunks(extracted_data):
    text_splitter=RecursiveCharacterTextSplitter(chunk_size=500,
                                                 chunk_overlap=50)
    text_chunks=text_splitter.split_documents(extracted_data)
    return text_chunks

text_chunks=create_chunks(extracted_data=documents)
#print("Length of Text Chunks: ", len(text_chunks))


def get_embedding_model():
    embedding_model=HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    return embedding_model

embedding_model=get_embedding_model()
DB_FAISS_PATH="vectorstore/db_faiss"
db=FAISS.from_documents(text_chunks, embedding_model)
db.save_local(DB_FAISS_PATH)



with open(".env", "w") as f:
    f.write("..........\n")

!huggingface-cli login

from huggingface_hub import InferenceClient
HF_TOKEN = os.environ.get("HF_TOKEN")
HUGGINGFACE_REPO_ID = "mistralai/Mistral-7B-Instruct-v0.3"
client = InferenceClient(model=HUGGINGFACE_REPO_ID, token=HF_TOKEN)
def generate_response(prompt):
    response = client.text_generation(prompt, max_new_tokens=512, temperature=0.5)
    return response
user_query = input("Write Query Here: ")
response = generate_response(user_query)
print("RESULT: ", response)

import os
from huggingface_hub import InferenceClient
HF_TOKEN = os.environ.get("HF_TOKEN")
if not HF_TOKEN:
    HF_TOKEN = input("Enter your Hugging Face API token: ").strip()
HUGGINGFACE_REPO_ID = "mistralai/Mistral-7B-Instruct-v0.3"
client = InferenceClient(model=HUGGINGFACE_REPO_ID, token=HF_TOKEN)
SYSTEM_PROMPT = """
You are a highly knowledgeable AI assistant. Provide concise, fact-based answers.
Avoid speculation and focus only on verified information.
Use structured responses where applicable.
"""
def generate_response(user_query):
    """
    Generates a response from the LLM with improved accuracy.
    """
    try:
        full_prompt = f"{SYSTEM_PROMPT}\n\nUser: {user_query}\nAssistant:"
        response = client.text_generation(
            full_prompt,
            max_new_tokens=512,
            temperature=0.3,
            top_p=0.9,
            repetition_penalty=1.2,
            stream=True
        )
        final_output = ""
        for chunk in response:
            print(chunk, end="", flush=True)
            final_output += chunk

        return final_output.strip()

    except Exception as e:
        return f"Error: {str(e)}"
user_query = input("\nWrite Query Here: ")
print("\nRESULT:\n")
final_result = generate_response(user_query)

!pip install streamlit pyngrok

# Commented out IPython magic to ensure Python compatibility.
# %%writefile medibot.py
# import os
# import streamlit as st
# from langchain.embeddings import HuggingFaceEmbeddings
# from langchain.chains import RetrievalQA
# from langchain_community.vectorstores import FAISS
# from langchain_core.prompts import PromptTemplate
# from langchain_huggingface import HuggingFaceEndpoint
# DB_FAISS_PATH="vectorstore/db_faiss"
# @st.cache_resource
# def get_vectorstore():
#     embedding_model=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')
#     db=FAISS.load_local(DB_FAISS_PATH, embedding_model, allow_dangerous_deserialization=True)
#     return db
# def set_custom_prompt(custom_prompt_template):
#     prompt=PromptTemplate(template=custom_prompt_template, input_variables=["context", "question"])
#     return prompt
# def load_llm(huggingface_repo_id, HF_TOKEN):
#     llm=HuggingFaceEndpoint(
#         repo_id=huggingface_repo_id,
#         temperature=0.5,
#         model_kwargs={"token":HF_TOKEN,
#                       "max_length":"512"}
#     )
#     return llm
# 
# def main():
#     st.title("Ask Chatbot!")
# 
#     if 'messages' not in st.session_state:
#         st.session_state.messages = []
# 
#     for message in st.session_state.messages:
#         st.chat_message(message['role']).markdown(message['content'])
# 
#     prompt=st.chat_input("Pass your prompt here")
# 
#     if prompt:
#         st.chat_message('user').markdown(prompt)
#         st.session_state.messages.append({'role':'user', 'content': prompt})
# 
#         CUSTOM_PROMPT_TEMPLATE = """
#                 Use the pieces of information provided in the context to answer user's question.
#                 If you dont know the answer, just say that you dont know, dont try to make up an answer.
#                 Dont provide anything out of the given context
# 
#                 Context: {context}
#                 Question: {question}
# 
#                 Start the answer directly. No small talk please.
#                 """
# 
#         HUGGINGFACE_REPO_ID="mistralai/Mistral-7B-Instruct-v0.3"
#         HF_TOKEN=os.environ.get("HF_TOKEN")
# 
#         try:
#             vectorstore=get_vectorstore()
#             if vectorstore is None:
#                 st.error("Failed to load the vector store")
# 
#             qa_chain=RetrievalQA.from_chain_type(
#                 llm=load_llm(huggingface_repo_id=HUGGINGFACE_REPO_ID, HF_TOKEN=HF_TOKEN),
#                 chain_type="stuff",
#                 retriever=vectorstore.as_retriever(search_kwargs={'k':3}),
#                 return_source_documents=True,
#                 chain_type_kwargs={'prompt':set_custom_prompt(CUSTOM_PROMPT_TEMPLATE)}
#             )
# 
#             response=qa_chain.invoke({'query':prompt})
# 
#             result=response["result"]
#             source_documents=response["source_documents"]
#             result_to_show=result+"\nSource Docs:\n"+str(source_documents)
#             #response="Hi, I am MediBot!"
#             st.chat_message('assistant').markdown(result_to_show)
#             st.session_state.messages.append({'role':'assistant', 'content': result_to_show})
# 
#         except Exception as e:
#             st.error(f"Error: {str(e)}")
# 
# if __name__ == "__main__":
#     main()

!ls

!ngrok

!ngrok authtoken 2rKtEJn7IVLFlRtdkh7PP6EBBUu_6toKutiViUvQffrNDRSyU

!ngrok

from pyngrok import ngrok

# !nohub streamlit run medi.py
!streamlit run medibot.py&>/dev/null&

!pgrep streamlit

public_url = ngrok.connect(8501)

public_url

